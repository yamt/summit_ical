BEGIN:VCALENDAR
DTSTART:20180519T080000
SUMMARY:OpenStack Summit Vancouver 2018: HPC / GPU / AI
BEGIN:VTIMEZONE
TZID:America/Vancouver
BEGIN:STANDARD
DTSTART:20171105T020000
RDATE:20181104T020000
TZNAME:PST
TZOFFSETFROM:-0700
TZOFFSETTO:-0800
END:STANDARD
BEGIN:DAYLIGHT
DTSTART:20180311T020000
TZNAME:PDT
TZOFFSETFROM:-0800
TZOFFSETTO:-0700
END:DAYLIGHT
END:VTIMEZONE
BEGIN:VEVENT
SUMMARY:Lessons Learned in Deploying OpenStack for HPC Users
DTSTART;VALUE=DATE-TIME:20180521T183500Z
DTEND;VALUE=DATE-TIME:20180521T191500Z
UID:21349@openstacksummitboston2017
DESCRIPTION:Modern research computing needs at academic institutions are e
 volving. While traditional HPC satisfies most workflows\, researchers seek
  sophisticated\, on-demand\, and self-service control of compute infrastru
 cture. Furthermore\, many also seek policy-compliant safe spaces to comput
 e on sensitive or protected data.\nTo cater to these users\, the Minnesota
  Supercomputing Institute deployed an OpenStack cloud called Stratus. In c
 ontrast to typical clouds\, Stratus does not manage internal infrastructur
 e\; rather\, Stratus complements bare-metal\, fully-managed HPC\, with a s
 elf-service model that can accommodate non-traditional computational and s
 torage needs with HPC-like performance.  \nThis talk describes the lesson
 s learned in launching a platform to support research with specific data-u
 se agreements\; and also issues concerning accountability\, risk acceptanc
 e\, and the role of project leadership when a large supercomputing facilit
 y deviates from its traditional base of support. 
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Ceph and the CERN HPC Infrastructure
DTSTART;VALUE=DATE-TIME:20180521T212000Z
DTEND;VALUE=DATE-TIME:20180521T220000Z
UID:20932@openstacksummitboston2017
DESCRIPTION:For the past 5 years\, CERN IT has employed Ceph technology to
  build scale-out storage for our OpenStack cloud. For the block and object
  storage use-cases\, with and without erasure coding\, Ceph has demonstrat
 ed to be flexible and scalable while being resiliant to infrastructure fai
 lures.\nBut as our computing requirements evolve\, the importance of a sca
 le-out HPC filesystem is emerging. We therefore present usage of CephFS --
  managed by Manila -- in production for our HPC and general IT infrastruct
 ure. We highlight the key metrics required by our users\, including POSIX 
 compliance\, small-file latency\, metadata throughput and scalability\, an
 d fault tolerance\, while showing the results of industry-standard and new
  microbenchmarks.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Call it real : Virtual GPUs in Nova
DTSTART;VALUE=DATE-TIME:20180521T221000Z
DTEND;VALUE=DATE-TIME:20180521T225000Z
UID:20802@openstacksummitboston2017
DESCRIPTION:GPUs in OpenStack? It’s a long-standing question. There are 
 many business cases for providing high-profile GPUs for every instance—n
 amely AI\, mining\, and desktop. Until Queens\, the only solution to expos
 e these devices to the guests was PCI passthrough in Nova—effective\, bu
 t wasteful in terms of resources.\nIn this session\, we’ll show you how\
 , as of Queens\, you can request virtual GPUs (vGPUs) for the libvirt/KVM 
 Nova driver. We'll share how the feature was discussed in the Nova communi
 ty to integrate with the new Placement service\, as well as how very diffe
 rent Nova drivers (Xen and libvirt/KVM) were able to cooperate to implemen
 t a major feature. We'll also discuss current feature limitations\, and sh
 are the roadmap for the next releases.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Three ARMed OpenStack - Building and Performance Testing OpenStack
  on ARM
DTSTART;VALUE=DATE-TIME:20180521T232000Z
DTEND;VALUE=DATE-TIME:20180522T000000Z
UID:20719@openstacksummitboston2017
DESCRIPTION:We've all heard great things about ARM. This new line of proce
 ssors and system architectures brings with it improved performance\, a wid
 er range of vendor offerings\, lower power usage\, and lower prices. This 
 new hardware design could bring OpenStack into new markets such as edge co
 mputing and container infrastructure! However\, is stock OpenStack ready f
 or ARM? Does OpenStack work on ARM as-is? How do the performance number co
 mpare?\nUsing the stock OpenStack source and the installation instructions
  available on OpenStack.org\, we built a Terraform configuration to deploy
  and performance test different hardware systems. This included ARM system
 s from Cavium\, Huawei\, and Qualcomm. As a baseline\, a multi-node Xeon s
 ystem built using the same Terraform configurations was performance tested
 \nWe'll be reviewing the modifications required to the stock\, upstream\, 
 OpenStack code based required for ARM support as well as why other OpenSta
 ck installers don't work on ARM.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Storage for Data Platforms
DTSTART;VALUE=DATE-TIME:20180522T002000Z
DTEND;VALUE=DATE-TIME:20180522T003000Z
UID:21402@openstacksummitboston2017
DESCRIPTION:Data volume is growing at an unprecedented rate\, often withou
 t consummate growth in computation needs. The world that lead to the emerg
 ence of the tenets laid out in the GFS and Map Reduce papers has seen a dr
 amatic transformation\, with over a decade of hardware improvements. Disag
 gregation of compute and storage is now commonplace\, championed by organi
 zations like Netflix\, Amazon\, and Airbnb. Pushing storage down a layer a
 nd treating it as an infrastructure service allows data platform teams to 
 focus on increasing the pace of innovation higher up the stack.\nEngineers
  building data platforms on OpenStack clouds expect a varity of storage se
 rvices. Object storage is increasingly becoming the centerpiece of data pl
 atforms\, as it enables in-situ analysis from elastic workload clusters\, 
 low cost\, and massive scalability. The presentation will detail object st
 orage centric data platform architectures\, and how to build rock solid st
 orage services suitable for data intensive applications.
LOCATION:Vancouver Convention Centre West - Level One - Lightning Talk The
 ater
END:VEVENT
BEGIN:VEVENT
SUMMARY:Ironing the clouds: A truly performant bare metal OpenStack!
DTSTART;VALUE=DATE-TIME:20180524T160000Z
DTEND;VALUE=DATE-TIME:20180524T164000Z
UID:21781@openstacksummitboston2017
DESCRIPTION:For some time now Ironic has been used for OpenStack bare met
 al provisioning. Bare metal clouds are often used for high end workloads s
 uch as scientific computing\, HPC\, ML/AI etc. These workloads typically 
 use InfiniBand network to maximize the full potential of the cloud.\nBy co
 mbining software-defined compute\, networking and storage\, the Commonweal
 th Scientific and Industrial Research Organization (CSIRO) of Australia an
 d Mellanox deliver API-driven access to native performance of the underlyi
 ng resources. This means that the researchers no longer have to make choic
 es between flexible and performant computing platforms and can easily appl
 y DevOps methodology to deploying and running the most demanding scientifi
 c applications\nIn this talk\,  CSIRO and Mellanox will present Ironic ov
 er InfiniBand implementation specifics and the way it was integrated with 
 OpenStack through real life deployment experience.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Can we boost more HPC performance? : Integrate IBM Power servers w
 ith GPUs to OpenStack Environment
DTSTART;VALUE=DATE-TIME:20180524T180000Z
DTEND;VALUE=DATE-TIME:20180524T184000Z
UID:21090@openstacksummitboston2017
DESCRIPTION:Deep learning(DL)\, High-Performance Computing(HPC)\, Big Data
  Analysis etc. has achieved remarkable progress in the recent years. To ma
 nage the heavy workloads\, we need high-speed communication between CPU-GP
 U and GPU-GPU. At the same time\, rapid enhancement of GPGPU programming i
 s required but its so complicated. We know utilizing OpenACC and CUDA Unif
 ied Memory will be helpful to reduce programming complexity but it require
 s more cost between CPU and GPU.So we choose IBM Power server with the ful
 l NVLink connectivity between CPU and GPU which efficiently reduces these 
 costs as possible. It also has high bandwidth path between CPU and GPU tha
 n PCIe Gen3 x16.Our main aim is to provide private cloud as in-house servi
 ce within our company after integrating IBM Power server to our internal O
 penStack environment. Then\, comparison is made among IBM Power servers wi
 th GPUs and other x86_64 servers with GPUs (like NVIDIA DGX-1 servers)
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Building Efficient HPC Clouds with MVAPICH2 and OpenStack over SR-
 IOV-enabled Heterogeneous Clusters
DTSTART;VALUE=DATE-TIME:20180524T185000Z
DTEND;VALUE=DATE-TIME:20180524T193000Z
UID:21292@openstacksummitboston2017
DESCRIPTION:Single Root I/O Virtualization (SR-IOV) technology has been st
 eadily gainingmomentum for high-performance interconnects. SR-IOV can deli
 ver near-nativeperformance but lacks locality-aware communication support.
  This talk presentsan efficient approach to building HPC clouds based on M
 VAPICH2 over OpenStackwith SR-IOV enabled virtualized heterogeneous cluste
 rs. We discusshigh-performance designs of the VM and container aware MVAPI
 CH2 library overOpenStack-based HPC Clouds with SR-IOV-enabled InfiniBand\
 , KNL\, and GPGPU. Thetalk will present a high-performance VM migration fr
 amework for MPIapplications on SR-IOV enabled InfiniBand clouds. A compreh
 ensive performanceevaluation with micro-benchmarks and HPC applications on
  NSF-supportedChameleon Cloud\, which is developed on OpenStack\, shows th
 at our design candeliver the near bare-metal performance. The MVAPICH2 ove
 r OpenStack softwarepackage presented in this talk is publicly available f
 romhttp://mvapich.cse.ohio-state.edu.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimized HPC/AI cloud with OpenStack acceleration service and com
 posable hardware
DTSTART;VALUE=DATE-TIME:20180524T205000Z
DTEND;VALUE=DATE-TIME:20180524T213000Z
UID:21215@openstacksummitboston2017
DESCRIPTION:Today data scientist is turning to cloud for AI and HPC worklo
 ads. However\, AI/HPC applications require high computational throughput w
 here generic cloud resources would not suffice. There is a strong demand f
 or OpenStack to support hardware accelerated devices in a dynamic model. \
 nIn this session\, we will introduce OpenStack Acceleration Service &ndash
 \; Cyborg\, which provides a management framework for accelerator devices 
 (e.g. FPGA\, GPU\, NVMe SSD). We will also discuss Rack Scale Design (RSD)
  technology and explain how physical hardware resources can be dynamically
  aggregated to meet the AI/HPC requirements. The ability to &ldquo\;compos
 e on the fly&rdquo\; with workload-optimized hardware and accelerator devi
 ces through an API allow data center managers to manage these resources in
  an efficient automated manner. \nWe will also introduce an enhanced telem
 etry solution with Gnnochi\, bandwidth discovery and smart scheduling\, by
  leveraging RSD technology\, for efficient workloads management in HPC/AI 
 cloud.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Building Big Data Analytics Data Lake with All-Flash Ceph
DTSTART;VALUE=DATE-TIME:20180524T213000Z
DTEND;VALUE=DATE-TIME:20180524T214000Z
UID:20905@openstacksummitboston2017
DESCRIPTION:Big data analytics data lake architecture aims to meet the sca
 lability requirement while adopting all-flash Ceph to address high I/O nee
 d. Leveraging Ceph RGW flexibility\, users can have multiple clusters runn
 ing different workloads concurrently with single back-end storage\, making
  it suitable for big data analytics. Big data query engines Hadoop Hive an
 d Presto is applied to represent user scenario. A comparable performance r
 esult is observed between disaggregated and hyper-converged architecture\,
  providing users an option to ensure cluster flexibility as well as optima
 l performance. Tests to evaluate NVMe performance is also conducted. By co
 mparing all-flash disks to spinning drives\, improvements in performance a
 nd resource utilization are observed.\nIn this session\, we will share per
 formance analysis of disaggregated architecture with all-flash Ceph. You w
 ill learn how parameters tuning affect performance and a suggested practic
 e to configure Ceph for big data.\n 
LOCATION:Vancouver Convention Centre West - Level One - Lightning Talk The
 ater
END:VEVENT
BEGIN:VEVENT
SUMMARY:Containers on Baremetal and Preemptible VMs at CERN and SKA
DTSTART;VALUE=DATE-TIME:20180524T223000Z
DTEND;VALUE=DATE-TIME:20180524T231000Z
UID:21148@openstacksummitboston2017
DESCRIPTION:CERN the European Laboratory for Nuclear Research and SKA the 
 Square Kilmeter Array are preparing the next generation of research infras
 tructure for the new large scale scientific instruments that will produce 
 new magnitudes of data. In Sydney OpenStack Summit we presented the collab
 oration and the platform that we plan to develop for scaling science.\nIn 
 this talk will present the work done related with Preemptible VMs and Cont
 ainers on Baremetal. Preemptible VMs are instances that use idle allocated
  resources in the infrastructure and can be terminated when this capacity 
 is required. Containers in Baremetal eliminate the virtualization overhead
  enabling the container full performance required for scientific workloads
 .\nWe will present the current state\, development and integration decisio
 ns and how these functionalities can be used in a common OpenStack infrast
 ructure.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Case Study: Large Scale Deployment for Machine Learning with High 
 Speed Storage
DTSTART;VALUE=DATE-TIME:20180524T234000Z
DTEND;VALUE=DATE-TIME:20180525T002000Z
UID:20791@openstacksummitboston2017
DESCRIPTION:Join our presentation to learn how you can build your cluster 
 for machine learning business. Machine learning and AI are obviously recen
 t new trend of technologies. NTT\, our big telecommunication company\, als
 o has its AI brand "Corevo". This presentation shares the experience\, how
  to build and manage our cloud-like computing infrastructure for our compa
 ny use case\, in which how we've been managing the full open source comput
 ing cluster environment including OpenStack components and container techn
 ologies.\nIn this talk\, we'd like to introduce our case study that a full
 -open sourced reference cluster model with Ansible and container orchestra
 tor automation. The environment built on GPU computation and high speed st
 orage\, in which we use Chainer and ChainerMN learning framework with many
  NVIDIA GPU nodes\, and attach perfectly scalable OpenStack Swift object s
 torage with file system APIs as the high speed data storage.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tuning OpenStack for HPC - experiences from TH-2
DTSTART;VALUE=DATE-TIME:20180525T003000Z
DTEND;VALUE=DATE-TIME:20180525T011000Z
UID:21079@openstacksummitboston2017
DESCRIPTION:In this session we will talk something about OpenStack based H
 PC ecosystem which is currently running on TianHe-2 Super computer. Curren
 tly we have thousands of nodes running in production mainly used for simul
 ation\, analysis\, and government security applications. In order to get b
 est performance for compute intensive application\, LXC is used as virtual
 ization for OpenStack\, lots of optimizations have been made and some patc
 hes are added to kernel and nova to support features like volume based LXC
 \, GPU passthrough for better performance.\nThis session will focus on how
  we tuning/modify Openstack and Operating system for large scale HPC\, and
  what's we gained by running HPC on OpenStack.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Ironing the clouds: A truly performant bare metal OpenStack!
DTSTART;VALUE=DATE-TIME:20180524T160000Z
DTEND;VALUE=DATE-TIME:20180524T164000Z
UID:21781@openstacksummitboston2017
DESCRIPTION:For some time now Ironic has been used for OpenStack bare met
 al provisioning. Bare metal clouds are often used for high end workloads s
 uch as scientific computing\, HPC\, ML/AI etc. These workloads typically 
 use InfiniBand network to maximize the full potential of the cloud.\nBy co
 mbining software-defined compute\, networking and storage\, the Commonweal
 th Scientific and Industrial Research Organization (CSIRO) of Australia an
 d Mellanox deliver API-driven access to native performance of the underlyi
 ng resources. This means that the researchers no longer have to make choic
 es between flexible and performant computing platforms and can easily appl
 y DevOps methodology to deploying and running the most demanding scientifi
 c applications\nIn this talk\,  CSIRO and Mellanox will present Ironic ov
 er InfiniBand implementation specifics and the way it was integrated with 
 OpenStack through real life deployment experience.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Can we boost more HPC performance? : Integrate IBM Power servers w
 ith GPUs to OpenStack Environment
DTSTART;VALUE=DATE-TIME:20180524T180000Z
DTEND;VALUE=DATE-TIME:20180524T184000Z
UID:21090@openstacksummitboston2017
DESCRIPTION:Deep learning(DL)\, High-Performance Computing(HPC)\, Big Data
  Analysis etc. has achieved remarkable progress in the recent years. To ma
 nage the heavy workloads\, we need high-speed communication between CPU-GP
 U and GPU-GPU. At the same time\, rapid enhancement of GPGPU programming i
 s required but its so complicated. We know utilizing OpenACC and CUDA Unif
 ied Memory will be helpful to reduce programming complexity but it require
 s more cost between CPU and GPU.So we choose IBM Power server with the ful
 l NVLink connectivity between CPU and GPU which efficiently reduces these 
 costs as possible. It also has high bandwidth path between CPU and GPU tha
 n PCIe Gen3 x16.Our main aim is to provide private cloud as in-house servi
 ce within our company after integrating IBM Power server to our internal O
 penStack environment. Then\, comparison is made among IBM Power servers wi
 th GPUs and other x86_64 servers with GPUs (like NVIDIA DGX-1 servers)
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Building Efficient HPC Clouds with MVAPICH2 and OpenStack over SR-
 IOV-enabled Heterogeneous Clusters
DTSTART;VALUE=DATE-TIME:20180524T185000Z
DTEND;VALUE=DATE-TIME:20180524T193000Z
UID:21292@openstacksummitboston2017
DESCRIPTION:Single Root I/O Virtualization (SR-IOV) technology has been st
 eadily gainingmomentum for high-performance interconnects. SR-IOV can deli
 ver near-nativeperformance but lacks locality-aware communication support.
  This talk presentsan efficient approach to building HPC clouds based on M
 VAPICH2 over OpenStackwith SR-IOV enabled virtualized heterogeneous cluste
 rs. We discusshigh-performance designs of the VM and container aware MVAPI
 CH2 library overOpenStack-based HPC Clouds with SR-IOV-enabled InfiniBand\
 , KNL\, and GPGPU. Thetalk will present a high-performance VM migration fr
 amework for MPIapplications on SR-IOV enabled InfiniBand clouds. A compreh
 ensive performanceevaluation with micro-benchmarks and HPC applications on
  NSF-supportedChameleon Cloud\, which is developed on OpenStack\, shows th
 at our design candeliver the near bare-metal performance. The MVAPICH2 ove
 r OpenStack softwarepackage presented in this talk is publicly available f
 romhttp://mvapich.cse.ohio-state.edu.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimized HPC/AI cloud with OpenStack acceleration service and com
 posable hardware
DTSTART;VALUE=DATE-TIME:20180524T205000Z
DTEND;VALUE=DATE-TIME:20180524T213000Z
UID:21215@openstacksummitboston2017
DESCRIPTION:Today data scientist is turning to cloud for AI and HPC worklo
 ads. However\, AI/HPC applications require high computational throughput w
 here generic cloud resources would not suffice. There is a strong demand f
 or OpenStack to support hardware accelerated devices in a dynamic model. \
 nIn this session\, we will introduce OpenStack Acceleration Service &ndash
 \; Cyborg\, which provides a management framework for accelerator devices 
 (e.g. FPGA\, GPU\, NVMe SSD). We will also discuss Rack Scale Design (RSD)
  technology and explain how physical hardware resources can be dynamically
  aggregated to meet the AI/HPC requirements. The ability to &ldquo\;compos
 e on the fly&rdquo\; with workload-optimized hardware and accelerator devi
 ces through an API allow data center managers to manage these resources in
  an efficient automated manner. \nWe will also introduce an enhanced telem
 etry solution with Gnnochi\, bandwidth discovery and smart scheduling\, by
  leveraging RSD technology\, for efficient workloads management in HPC/AI 
 cloud.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Building Big Data Analytics Data Lake with All-Flash Ceph
DTSTART;VALUE=DATE-TIME:20180524T213000Z
DTEND;VALUE=DATE-TIME:20180524T214000Z
UID:20905@openstacksummitboston2017
DESCRIPTION:Big data analytics data lake architecture aims to meet the sca
 lability requirement while adopting all-flash Ceph to address high I/O nee
 d. Leveraging Ceph RGW flexibility\, users can have multiple clusters runn
 ing different workloads concurrently with single back-end storage\, making
  it suitable for big data analytics. Big data query engines Hadoop Hive an
 d Presto is applied to represent user scenario. A comparable performance r
 esult is observed between disaggregated and hyper-converged architecture\,
  providing users an option to ensure cluster flexibility as well as optima
 l performance. Tests to evaluate NVMe performance is also conducted. By co
 mparing all-flash disks to spinning drives\, improvements in performance a
 nd resource utilization are observed.\nIn this session\, we will share per
 formance analysis of disaggregated architecture with all-flash Ceph. You w
 ill learn how parameters tuning affect performance and a suggested practic
 e to configure Ceph for big data.\n 
LOCATION:Vancouver Convention Centre West - Level One - Lightning Talk The
 ater
END:VEVENT
BEGIN:VEVENT
SUMMARY:Containers on Baremetal and Preemptible VMs at CERN and SKA
DTSTART;VALUE=DATE-TIME:20180524T223000Z
DTEND;VALUE=DATE-TIME:20180524T231000Z
UID:21148@openstacksummitboston2017
DESCRIPTION:CERN the European Laboratory for Nuclear Research and SKA the 
 Square Kilmeter Array are preparing the next generation of research infras
 tructure for the new large scale scientific instruments that will produce 
 new magnitudes of data. In Sydney OpenStack Summit we presented the collab
 oration and the platform that we plan to develop for scaling science.\nIn 
 this talk will present the work done related with Preemptible VMs and Cont
 ainers on Baremetal. Preemptible VMs are instances that use idle allocated
  resources in the infrastructure and can be terminated when this capacity 
 is required. Containers in Baremetal eliminate the virtualization overhead
  enabling the container full performance required for scientific workloads
 .\nWe will present the current state\, development and integration decisio
 ns and how these functionalities can be used in a common OpenStack infrast
 ructure.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Case Study: Large Scale Deployment for Machine Learning with High 
 Speed Storage
DTSTART;VALUE=DATE-TIME:20180524T234000Z
DTEND;VALUE=DATE-TIME:20180525T002000Z
UID:20791@openstacksummitboston2017
DESCRIPTION:Join our presentation to learn how you can build your cluster 
 for machine learning business. Machine learning and AI are obviously recen
 t new trend of technologies. NTT\, our big telecommunication company\, als
 o has its AI brand "Corevo". This presentation shares the experience\, how
  to build and manage our cloud-like computing infrastructure for our compa
 ny use case\, in which how we've been managing the full open source comput
 ing cluster environment including OpenStack components and container techn
 ologies.\nIn this talk\, we'd like to introduce our case study that a full
 -open sourced reference cluster model with Ansible and container orchestra
 tor automation. The environment built on GPU computation and high speed st
 orage\, in which we use Chainer and ChainerMN learning framework with many
  NVIDIA GPU nodes\, and attach perfectly scalable OpenStack Swift object s
 torage with file system APIs as the high speed data storage.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tuning OpenStack for HPC - experiences from TH-2
DTSTART;VALUE=DATE-TIME:20180525T003000Z
DTEND;VALUE=DATE-TIME:20180525T011000Z
UID:21079@openstacksummitboston2017
DESCRIPTION:In this session we will talk something about OpenStack based H
 PC ecosystem which is currently running on TianHe-2 Super computer. Curren
 tly we have thousands of nodes running in production mainly used for simul
 ation\, analysis\, and government security applications. In order to get b
 est performance for compute intensive application\, LXC is used as virtual
 ization for OpenStack\, lots of optimizations have been made and some patc
 hes are added to kernel and nova to support features like volume based LXC
 \, GPU passthrough for better performance.\nThis session will focus on how
  we tuning/modify Openstack and Operating system for large scale HPC\, and
  what's we gained by running HPC on OpenStack.
LOCATION:Vancouver Convention Centre West - Level Three - Room 301
END:VEVENT
END:VCALENDAR
